# This is a basic workflow to help you get started with Actions

name: Scrape

# Controls when the workflow will run
on:
  
  schedule:
    - cron:  '15 4 * * *'   
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
    inputs:
      specific_scraper:
        type: choice
        description: Which scraper to run?
        options: 
        - bills
        - events
        - people
      window:
        description: How many days to scrape?
        type: string

concurrency:
  group: chicago-scraper
    
# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  scrape:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v3
      - name: install dependencies
        run: |
          sudo apt-get install -y libgdal-dev
          pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: run scraper without window
        if: ${{ !inputs.window }}
        env:
          DJANGO_SETTINGS_MODULE: pupa.settings
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          DATABASE_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: pupa update chicago ${{ inputs.specific_scraper }} 
        
      - name: run scraper with window
        if: ${{ inputs.window }}
        env:
          DJANGO_SETTINGS_MODULE: pupa.settings
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          DATABASE_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: pupa update chicago ${{ inputs.specific_scraper }} window=${{ inputs.window }}
        

      - name: keepalive
        uses: gautamkrishnar/keepalive-workflow@v1
  index:
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: update search index
        uses: michcio1234/heroku-run@0.1.1
        with:
          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}
          heroku_email: ${{ secrets.HEROKU_ACCOUNT }}
          heroku_app_name: ${{ secrets.HEROKU_APP }}
          command: python manage.py update_index --batch-size=50 --age=1

  export:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    needs: scrape
    
    steps:
      - uses: actions/checkout@v3
      - name: install dependencies
        run: pip install "db-to-sqlite[postgresql] @ https://github.com/sgraaf/db-to-sqlite/archive/refs/heads/main.zip"
      - name: export
        env:
          DATABASE_URL: ${{ secrets.DB_CONNECTION_STRING }}
        run: |
          db-to-sqlite $(echo $DATABASE_URL) chicago_council.db --table-name-pattern opencivicdata_*
          zip chicago_council.db.zip chicago_council.db
      - name: Push data
        uses: WebFreak001/deploy-nightly@v1.1.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # automatically provided by github actions
        with:
          upload_url: https://uploads.github.com/repos/datamade/chicago-council-scrapers/releases/80533645/assets{?name,label}
          release_id: 80533645 # same as above (id can just be taken out the upload_url, it's used to find old releases)
          asset_path: ./chicago_council.db.zip # path to archive to upload
          asset_name: chicago_council.db.zip # name to upload the release as, use $$ to insert date (YYYYMMDD) and 6 letter commit hash
          asset_content_type: application/zip # required by GitHub API
